#!/usr/bin/env bash
# set -ex

export PORT=1027

ranges=("32" "128" "512" "1536")
max_batch_total_tokens_vals=("8700" "7500" "8100" "8100")
model_name=meta-llama/Llama-2-7b-hf
results_home=./result
mkdir -p ${results_home}

function start_model_server {
    local max_batch_total_tokens=$1

    model_name=$model_name \
    max_batch_total_tokens=$max_batch_total_tokens \
    PORT=$PORT \
    ../launch_scripts/launch_tgi_gaudi \
    >> tgi_log 2>&1 &
    
    while [ "$(curl -s http://localhost:${PORT}/info | grep $model_name | wc -l)" -eq 0 ]; do
        echo 'Waiting for server to be ready'
        sleep 10
    done
    echo "model server started on port $PORT, max_batch_total_tokens $max_batch_total_tokens"
}

function kill_model_server {
    echo 'killing model server'
    ps aux | grep 'text-generation-launcher' | awk '{print $2}' | xargs kill -9
    ps aux | grep 'text-generation-router' | awk '{print $2}' | xargs kill -9
    ps aux | grep 'text-generation-server' | awk '{print $2}' | xargs kill -9
    wait
}

trap kill_model_server EXIT

# Catch OOMs early.
for i in ${!ranges[@]}; do
    range=${ranges[$i]}
    max_batch_total_tokens=${max_batch_total_tokens_vals[$i]}
    

    echo "range $range max_batch_total_tokens $max_batch_total_tokens"

    start_model_server $max_batch_total_tokens

    pushd ..
        ./benchmark_throughput.py \
            --port $PORT \
            --backend HfTextGenerationInference \
            --random_prompt_lens_mean 512 \
            --random_prompt_lens_range 0 \
            --random_prompt_count 10 \
            --gen_random_prompts \
            --fixed_max_tokens $range
    popd

    kill_model_server
done

# Run real test
for i in ${!ranges[@]}; do
    range=${ranges[$i]}
    max_batch_total_tokens=${max_batch_total_tokens_vals[$i]}
    
    echo "range $range max_batch_total_tokens $max_batch_total_tokens"
    QPS=4

    start_model_server $max_batch_total_tokens

    pushd ..
        ./benchmark_throughput.py \
            --port $PORT \
            --backend HfTextGenerationInference \
            --random_prompt_lens_mean 256 \
            --random_prompt_lens_range 256 \
            --random_prompt_count 100 \
            --gen_random_prompts \
            --variable_response_lens_mean 128 \
            --variable_response_lens_range $range \
            --variable_response_lens_distribution capped_exponential \
            --allow_variable_generation_length \
            --distribution poisson \
            --qps $QPS \
            --log_latencies \
            --fail_on_response_failure \
            --results_filename ${results_home}/tgi_gaudi_qps_${QPS}_range_${range}_$(date '+%Y-%m-%d_%H-%M-%S').log
    popd

    kill_model_server
done
